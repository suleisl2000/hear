{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fgVWTMK9SNz"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Quick start with Hugging Face\n",
        "\n",
        "\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://colab.research.google.com/github/google-health/hear/blob/master/notebooks/quick_start_with_hugging_face.ipynb\"\u003e\n",
        "      \u003cimg alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"\u003e\u003cbr\u003e Run in Google Colab\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e  \n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://github.com/google-health/hear/blob/master/notebooks/quick_start_with_hugging_face.ipynb\"\u003e\n",
        "      \u003cimg alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"\u003e\u003cbr\u003e View on GitHub\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://huggingface.co/google/hear\"\u003e\n",
        "      \u003cimg alt=\"Hugging Face logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"\u003e\u003cbr\u003e View on Hugging Face\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n",
        "\n",
        "This Colab notebook provides a basic usage example of the HeAR encoder that generates a machine learning representation (known as \"embeddings\") from health-related sounds (2-second audio clips sampled at 16kHz). These embeddings can be used to develop custom machine learning models for health acoustic use-cases with less data and compute compared to traditional model development methods.\n",
        "\n",
        " Learn more about embeddings and their benefits at [this page](https://developers.google.com/health-ai-developer-foundations/hear)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uZFXCSuqr1V"
      },
      "source": [
        "# Authenticate with HuggingFace, skip if you have a HF_TOKEN secret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XpCla68-Iol"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub.utils import HfFolder\n",
        "\n",
        "if HfFolder.get_token() is None:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWckxRXiqc3L"
      },
      "source": [
        "## Load and play cough audio recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2vEauhtvTan"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 16000  # Samples per second (Hz)\n",
        "CLIP_DURATION = 2    # Duration of the audio clip in seconds\n",
        "CLIP_LENGTH = SAMPLE_RATE * CLIP_DURATION  # Total number of samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j81KORIqloq-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "from scipy import signal\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "\n",
        "def resample_audio_and_convert_to_mono(\n",
        "    audio_array: np.ndarray,\n",
        "    sampling_rate: float,\n",
        "    new_sampling_rate: float = SAMPLE_RATE,\n",
        "  ) -\u003e np.ndarray:\n",
        "  \"\"\"\n",
        "  Resamples an audio array to 16kHz and converts it to mono if it has multiple channels.\n",
        "\n",
        "  Args:\n",
        "    audio_array: A numpy array representing the audio data.\n",
        "    sampling_rate: The original sampling rate of the audio.\n",
        "    new_sampling_rate: Target sampling rate.\n",
        "\n",
        "  Returns:\n",
        "    resampled_audio_mono: A numpy array representing the resampled mono audio at 16kHz.\n",
        "  \"\"\"\n",
        "  # Convert to mono if it's multi-channel\n",
        "  if audio_array.ndim \u003e 1:\n",
        "    audio_mono = np.mean(audio_array, axis=1)\n",
        "  else:\n",
        "    audio_mono = audio_array\n",
        "\n",
        "  # Resample\n",
        "  original_sample_count = audio_mono.shape[0]\n",
        "  new_sample_count = int(round(original_sample_count * (new_sampling_rate / sampling_rate)))\n",
        "  resampled_audio_mono = signal.resample(audio_mono, new_sample_count)\n",
        "\n",
        "  return resampled_audio_mono\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1Lw92M1b7Dd"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://upload.wikimedia.org/wikipedia/commons/b/be/Woman_coughing_three_times.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W7BMKk9cerB"
      },
      "outputs": [],
      "source": [
        "# Load file\n",
        "with open('Woman_coughing_three_times.wav', 'rb') as f:\n",
        "  original_sampling_rate, audio_array = wavfile.read(f)\n",
        "\n",
        "print(f\"Sample Rate: {original_sampling_rate} Hz\")\n",
        "print(f\"Data Shape: {audio_array.shape}\")\n",
        "print(f\"Data Type: {audio_array.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1l-eFOh0mq1r"
      },
      "outputs": [],
      "source": [
        "audio_array = resample_audio_and_convert_to_mono(audio_array, original_sampling_rate, SAMPLE_RATE)\n",
        "display(Audio(audio_array, rate=SAMPLE_RATE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJIdGmpFqhi8"
      },
      "source": [
        "## Compute embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhk4GKkwsAMj"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import from_pretrained_keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# This index corresponds to a cough and was determined by hand. In practice, you\n",
        "# would need a detector.\n",
        "START = 0\n",
        "\n",
        "# Add batch dimension\n",
        "input_tensor = np.expand_dims(audio_array[START: START + CLIP_LENGTH], axis=0)\n",
        "\n",
        "# Load the model directly from Hugging Face Hub\n",
        "loaded_model = from_pretrained_keras(\"google/hear\")\n",
        "\n",
        "# Call inference\n",
        "infer = lambda audio_array: loaded_model.signatures[\"serving_default\"](x=audio_array)\n",
        "output = infer(tf.constant(input_tensor, dtype=tf.float32))\n",
        "\n",
        "# Extract the embedding vector\n",
        "embedding_vector = output['output_0'].numpy().flatten()\n",
        "print(\"Size of embedding vector:\", len(embedding_vector))\n",
        "\n",
        "# Plot the embedding vector\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(embedding_vector)\n",
        "plt.title('Embedding Vector')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHTxQttKYNpa"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/hear/blob/master/notebooks) to learn what else you can do with the model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "quick_start_with_hugging_face.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
